{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5076e401",
   "metadata": {},
   "source": [
    "# Transfer Learning Goal\n",
    "\n",
    "* **model_0** - Create a pretrained EfficientNetB0 model with 10 percent data\n",
    "  * use 10 percent data - https://storage.googleapis.com/ztm_tf_course/food_vision/10_food_classes_10_percent.zip\n",
    "* **model_1** - Create a pretrained EfficientNetB0 on 1 percent data with data augmentation\n",
    "  * use 1 percent data  - https://storage.googleapis.com/ztm_tf_course/food_vision/10_food_classes_1_percent.zip\n",
    "* **model_2** - Createa a pretrained EfficientNetB0 model with 10 percent data and data augmentation layer. \n",
    "* **model_3** - Create a model from feature extraction from model 2 and fine tune the model with last 10 layer unfrozen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47c326c",
   "metadata": {},
   "source": [
    "## Import Dependency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab523d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget https://github.com/rkumar-bengaluru/pythonbyexample/raw/main/tensorflow/utilities.py\n",
    "from utilities import download_file,unzip_file\n",
    "import os\n",
    "import datetime \n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import random\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers.experimental.preprocessing import RandomContrast,RandomCrop,RandomWidth\n",
    "from tensorflow.keras.layers.experimental.preprocessing import RandomFlip,RandomRotation,RandomHeight,RandomZoom\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras.layers import Input,GlobalAveragePooling2D,Dense\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import TensorBoard,ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1be8ab",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f0479b45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file already exists 10_food_classes_10_percent.zip\n",
      "file already exists 10_food_classes_1_percent.zip\n",
      "Found 750 files belonging to 10 classes.\n",
      "Found 2500 files belonging to 10 classes.\n",
      "Found 70 files belonging to 10 classes.\n",
      "Found 2500 files belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "# constants\n",
    "ten_percent_data_link = 'https://storage.googleapis.com/ztm_tf_course/food_vision/10_food_classes_10_percent.zip'\n",
    "one_percent_data_link = 'https://storage.googleapis.com/ztm_tf_course/food_vision/10_food_classes_1_percent.zip'\n",
    "hundred_percent_data_link = 'https://storage.googleapis.com/ztm_tf_course/food_vision/10_food_classes_all_data.zip'\n",
    "tensorboard_dir_name = 'transfer_learning'\n",
    "modelcheckpoint_dir_name = 'modelcheckpoing'\n",
    "\n",
    "# train directories\n",
    "all_train_dir         = hundred_percent_data_link.split('/')[-1].split('.')[0] + '/train/'\n",
    "ten_percent_train_dir = ten_percent_data_link.split('/')[-1].split('.')[0] + '/train/'\n",
    "one_percent_train_dir = one_percent_data_link.split('/')[-1].split('.')[0] + '/train/'\n",
    "# test directories\n",
    "all_test_dir          = hundred_percent_data_link.split('/')[-1].split('.')[0] + '/test/'\n",
    "ten_percent_test_dir  = ten_percent_data_link.split('/')[-1].split('.')[0] + '/test/'\n",
    "one_percent_test_dir  = one_percent_data_link.split('/')[-1].split('.')[0] + '/test/'\n",
    "# target image size\n",
    "TARGET_IMG_SIZE       = (224,224)\n",
    "# input shape\n",
    "INPUT_SHAPE           = (224,224,3)\n",
    "# matplot lib fig size\n",
    "FIG_SIZE              = (10,7)\n",
    "# labels size\n",
    "LABELS_SIZE           = 10\n",
    "# no of epochs\n",
    "INITIAL_EPOCHS        = 5\n",
    "\n",
    "# download both data sets\n",
    "download_file(ten_percent_data_link)\n",
    "download_file(one_percent_data_link)\n",
    "\n",
    "# unzip data directories\n",
    "unzip_file(ten_percent_data_link)\n",
    "unzip_file(one_percent_data_link)\n",
    "\n",
    "# prepare all train and test data\n",
    "train_all_data = image_dataset_from_directory(directory=all_train_dir,\n",
    "                                                 image_size=TARGET_IMG_SIZE,\n",
    "                                                     label_mode='categorical')\n",
    "test_all_data  = image_dataset_from_directory(directory=all_test_dir,\n",
    "                                                     image_size=TARGET_IMG_SIZE,\n",
    "                                                     label_mode='categorical')\n",
    "# prepare 10% train and test data\n",
    "train_ten_percent_data = image_dataset_from_directory(directory=ten_percent_train_dir,\n",
    "                                                 image_size=TARGET_IMG_SIZE,\n",
    "                                                     label_mode='categorical')\n",
    "test_ten_percent_data  = image_dataset_from_directory(directory=ten_percent_test_dir,\n",
    "                                                     image_size=TARGET_IMG_SIZE,\n",
    "                                                     label_mode='categorical')\n",
    "# prepare 1% train and test data\n",
    "train_one_percent_data  = image_dataset_from_directory(directory=one_percent_train_dir,\n",
    "                                                      image_size=TARGET_IMG_SIZE,\n",
    "                                                     label_mode='categorical')\n",
    "test_one_percent_data   = image_dataset_from_directory(directory=one_percent_test_dir,\n",
    "                                                      image_size=TARGET_IMG_SIZE,\n",
    "                                                     label_mode='categorical')\n",
    "# create the data augmentation layer.\n",
    "data_augmentation_layer = Sequential([\n",
    "    RandomContrast(0.2),\n",
    "    RandomWidth(0.2),\n",
    "    RandomFlip(mode='horizontal'),\n",
    "    RandomRotation(0.2),\n",
    "    RandomHeight(0.2),\n",
    "    RandomZoom(0.2)\n",
    "],name='data_augmentation_layer')   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b580fbb8",
   "metadata": {},
   "source": [
    "## Create Model 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9146267b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'INPUT_SHAPE' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m base_model\u001b[38;5;241m.\u001b[39mtrainable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# step 3 - create inputs in our model\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m inputs \u001b[38;5;241m=\u001b[39m Input(shape\u001b[38;5;241m=\u001b[39m\u001b[43mINPUT_SHAPE\u001b[49m,name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_layer\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# step 4 - if using Resnet50V2 we need rescale.\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# x = tf.keras.layers.Rescaling(scale=SCALE)(inputs)\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# step 5 - pass the inputs to base model\u001b[39;00m\n\u001b[0;32m     16\u001b[0m x \u001b[38;5;241m=\u001b[39m base_model(inputs)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'INPUT_SHAPE' is not defined"
     ]
    }
   ],
   "source": [
    "experiment_name_0 = 'model_0'\n",
    "\n",
    "# step 1 - create a model with tensorflow applications\n",
    "base_model = EfficientNetB0(include_top=False)\n",
    "\n",
    "# step 2 - freeze the base model\n",
    "base_model.trainable = False\n",
    "\n",
    "# step 3 - create inputs in our model\n",
    "inputs = Input(shape=INPUT_SHAPE,name='input_layer')\n",
    "\n",
    "# step 4 - if using Resnet50V2 we need rescale.\n",
    "# x = tf.keras.layers.Rescaling(scale=SCALE)(inputs)\n",
    "\n",
    "# step 5 - pass the inputs to base model\n",
    "x = base_model(inputs)\n",
    "print(f'shape after passing the inputs to base model {x.shape}')\n",
    "\n",
    "# step 6 - Average Pooling\n",
    "x = GlobalAveragePooling2D(name='global_avaerage_pooling_layer')(x)\n",
    "#x = GlobalAveragePooling2D(name='global_average_pooling_layer')(x)\n",
    "print(f'shape after passing through average pool {x.shape}')\n",
    "\n",
    "# step 7 - create the output layer\n",
    "outputs = Dense(LABELS_SIZE,activation='softmax',name='output_layer')(x)\n",
    "\n",
    "# step 8 - create the model\n",
    "model_0 = tf.keras.Model(inputs,outputs)\n",
    "# compile the model\n",
    "model_0.compile(loss=CategoricalCrossentropy(),\n",
    "               optimizer=Adam(),\n",
    "               metrics=['accuracy'])\n",
    "# tensor board callback\n",
    "log_dir = tensorboard_dir_name + '/' + experiment_name_0 + '/' + datetime.datetime.now().strftime('%Y%m%d-%H%M%S')\n",
    "tensorboard_callback = TensorBoard(log_dir=log_dir)\n",
    "# ModelCheckpoint\n",
    "modelcheckpoint_path_1 = f'{modelcheckpoint_dir_name}/{experiment_name_0}/checkpoint.ckpt' \n",
    "checkoutpoint_path=\"modelcheckpoing/model_0/checkpoint.ckpt\"\n",
    "modelcheckpoint_callback = ModelCheckpoint(filepath=checkoutpoint_path, \n",
    "                                           save_weights_only=True,\n",
    "                                           save_best_only=False,\n",
    "                                           save_freq='epoch',\n",
    "                                           verbose=1)\n",
    "model_0.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0a7651",
   "metadata": {},
   "source": [
    "## Create Model 1 - With Data Augmentation Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a685a123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_layer_1 (InputLayer)  [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " data_augmentation_layer (Se  (None, 224, 224, 3)      0         \n",
      " quential)                                                       \n",
      "                                                                 \n",
      " efficientnetb0 (Functional)  (None, None, None, 1280)  4049571  \n",
      "                                                                 \n",
      " global_average_pooling_laye  (None, 1280)             0         \n",
      " r (GlobalAveragePooling2D)                                      \n",
      "                                                                 \n",
      " output_layer (Dense)        (None, 10)                12810     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,062,381\n",
      "Trainable params: 12,810\n",
      "Non-trainable params: 4,049,571\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "experiment_name_1 = 'model_1'\n",
    "# step 1 - create the base model\n",
    "base_model_1 = EfficientNetB0(include_top=False)\n",
    "# step 2 - Make the base model untrainable.\n",
    "base_model_1.trainable = False\n",
    "# step 3 - Create the input layer\n",
    "inputs_1 = Input(shape=INPUT_SHAPE,name='input_layer_1')\n",
    "# step 4 - create the data augmentation layer\n",
    "x = data_augmentation_layer(inputs_1)\n",
    "# step 5 - pass through the inputs to base model.\n",
    "x = base_model_1(x,training=False)\n",
    "# step 6 - pass throug the global max pool\n",
    "x = GlobalAveragePooling2D(name='global_average_pooling_layer')(x)\n",
    "# step 7 - define output layer\n",
    "outputs = Dense(LABELS_SIZE,activation='softmax',name='output_layer')(x)\n",
    "# step 8 - create the model\n",
    "model_1 = tf.keras.Model(inputs_1,outputs)\n",
    "# step 9 - compile the model.\n",
    "model_1.compile(loss=CategoricalCrossentropy(),\n",
    "               optimizer=Adam(),\n",
    "               metrics=['accuracy'])\n",
    "# step 10 - create tensorboard call back\n",
    "log_dir = tensorboard_dir_name + '/' + experiment_name_1 + '/' + datetime.datetime.now().strftime('%Y%m%d-%H%M%S')\n",
    "tensorboard_callback_for_model_1 = TensorBoard(log_dir=log_dir)\n",
    "# step 11 - create model checkpoint callback\n",
    "modelcheckpoint_path_1 = f'{modelcheckpoint_dir_name}/{experiment_name_1}/checkpoint.ckpt' \n",
    "modelcheckpoint_callback_for_model_1 = ModelCheckpoint(filepath=modelcheckpoint_path_1, \n",
    "                                           save_weights_only=True,\n",
    "                                           save_best_only=False,\n",
    "                                           save_freq='epoch',\n",
    "                                           verbose=1)\n",
    "# step 10 - print the model summary\n",
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d04579f",
   "metadata": {},
   "source": [
    "## Create Model 2 - With 10 percent data and Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d9482b7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_17\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_layer_1 (InputLayer)  [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " data_augmentation_layer (Se  (None, 224, 224, 3)      0         \n",
      " quential)                                                       \n",
      "                                                                 \n",
      " efficientnetb0 (Functional)  (None, None, None, 1280)  4049571  \n",
      "                                                                 \n",
      " global_average_pooling_laye  (None, 1280)             0         \n",
      " r (GlobalAveragePooling2D)                                      \n",
      "                                                                 \n",
      " output_layer (Dense)        (None, 10)                12810     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,062,381\n",
      "Trainable params: 12,810\n",
      "Non-trainable params: 4,049,571\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "experiment_name_2 = 'model_2'\n",
    "# step 1 - create the base model\n",
    "base_model_2 = EfficientNetB0(include_top=False)\n",
    "# step 2 - Make the base model untrainable.\n",
    "base_model_2.trainable = False\n",
    "# step 3 - Create the input layer\n",
    "inputs_2 = Input(shape=INPUT_SHAPE,name='input_layer_1')\n",
    "# step 4 - create the data augmentation layer\n",
    "x = data_augmentation_layer(inputs_2)\n",
    "# step 5 - pass through the inputs to base model.\n",
    "x = base_model_2(x,training=False)\n",
    "# step 6 - pass throug the global max pool\n",
    "x = GlobalAveragePooling2D(name='global_average_pooling_layer')(x)\n",
    "# step 7 - define output layer\n",
    "outputs = Dense(LABELS_SIZE,activation='softmax',name='output_layer')(x)\n",
    "# step 8 - create the model\n",
    "model_2 = tf.keras.Model(inputs_2,outputs)\n",
    "# step 9 - compile the model.\n",
    "model_2.compile(loss=CategoricalCrossentropy(),\n",
    "               optimizer=Adam(),\n",
    "               metrics=['accuracy'])\n",
    "# step 10 - create tensorboard call back\n",
    "log_dir = tensorboard_dir_name + '/' + experiment_name_2 + '/' + datetime.datetime.now().strftime('%Y%m%d-%H%M%S')\n",
    "tensorboard_callback_for_model_2 = TensorBoard(log_dir=log_dir)\n",
    "# step 11 - create model checkpoint callback\n",
    "modelcheckpoint_path_2 = f'{modelcheckpoint_dir_name}/{experiment_name_2}/checkpoint.ckpt' \n",
    "modelcheckpoint_callback_for_model_2 = ModelCheckpoint(filepath=modelcheckpoint_path_2, \n",
    "                                           save_weights_only=True,\n",
    "                                           save_best_only=False,\n",
    "                                           save_freq='epoch',\n",
    "                                           verbose=1)\n",
    "# step 10 - print the model summary\n",
    "model_2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547885a3",
   "metadata": {},
   "source": [
    "## Fit All the Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84fa1f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit model 0\n",
    "history_0 = model_0.fit(train_ten_percent_data,epochs=5,\n",
    "                     steps_per_epoch=len(train_ten_percent_data),\n",
    "                     validation_data=test_ten_percent_data,\n",
    "                     validation_steps=int(0.25 *len(test_ten_percent_data)),\n",
    "                     callbacks=[tensorboard_callback,modelcheckpoint_callback])\n",
    "# fit model 1\n",
    "history_1 = model_1.fit(train_one_percent_data,epochs=5,\n",
    "                     steps_per_epoch=len(train_one_percent_data),\n",
    "                     validation_data=test_one_percent_data,\n",
    "                     validation_steps=int(0.25 *len(test_one_percent_data)),\n",
    "                     callbacks=[tensorboard_callback_for_model_1,modelcheckpoint_callback_for_model_1])\n",
    "# fit model 1\n",
    "history_2 = model_2.fit(train_ten_percent_data,epochs=5,\n",
    "                     steps_per_epoch=len(train_ten_percent_data),\n",
    "                     validation_data=test_ten_percent_data,\n",
    "                     validation_steps=int(0.25 *len(test_ten_percent_data)),\n",
    "                     callbacks=[tensorboard_callback_for_model_2,modelcheckpoint_callback_for_model_2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4f24bb",
   "metadata": {},
   "source": [
    "## Model 3 - Fine Tuning from feature extraction from Model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "278bc332",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'EfficientNetB0' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m experiment_name_3 \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_3\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# step 1 - create the base model\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m base_model_3 \u001b[38;5;241m=\u001b[39m \u001b[43mEfficientNetB0\u001b[49m(include_top\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# step 2 - Make the base model untrainable.\u001b[39;00m\n\u001b[0;32m      5\u001b[0m base_model_3\u001b[38;5;241m.\u001b[39mtrainable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'EfficientNetB0' is not defined"
     ]
    }
   ],
   "source": [
    "experiment_name_3 = 'model_3'\n",
    "# step 1 - create the base model\n",
    "base_model_3 = EfficientNetB0(include_top=False)\n",
    "# step 2 - Make the base model untrainable.\n",
    "base_model_3.trainable = False\n",
    "# step 3 - Create the input layer\n",
    "inputs_3 = Input(shape=INPUT_SHAPE,name='input_layer_1')\n",
    "# step 4 - create the data augmentation layer\n",
    "x = data_augmentation_layer(inputs_3)\n",
    "# step 5 - pass through the inputs to base model.\n",
    "x = base_model_3(x,training=False)\n",
    "# step 6 - pass throug the global max pool\n",
    "x = GlobalAveragePooling2D(name='global_average_pooling_layer')(x)\n",
    "# step 7 - define output layer\n",
    "outputs = Dense(LABELS_SIZE,activation='softmax',name='output_layer')(x)\n",
    "# step 8 - create the model\n",
    "model_3 = tf.keras.Model(inputs_3,outputs)\n",
    "# step 9 - load weights from previous model check point file.\n",
    "model_3.load_weights(modelcheckpoint_path_2)\n",
    "\n",
    "# step 10 - create tensorboard call back\n",
    "log_dir = tensorboard_dir_name + '/' + experiment_name_3 + '/' + datetime.datetime.now().strftime('%Y%m%d-%H%M%S')\n",
    "tensorboard_callback_for_model_3 = TensorBoard(log_dir=log_dir)\n",
    "# step 11 - create model checkpoint callback\n",
    "modelcheckpoint_path_3 = f'{modelcheckpoint_dir_name}/{experiment_name_3}/checkpoint.ckpt' \n",
    "modelcheckpoint_callback_for_model_2 = ModelCheckpoint(filepath=modelcheckpoint_path_3, \n",
    "                                           save_weights_only=True,\n",
    "                                           save_best_only=False,\n",
    "                                           save_freq='epoch',\n",
    "                                           verbose=1)\n",
    "# step 10 - print the model summary\n",
    "model_3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1849453a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unfreeze top 10 layers in EfficientNetB0 for learning.\n",
    "def make_model_trainable(model,no_of_layers):\n",
    "    model.trainable = True\n",
    "    for layer in model.layers[:-10]:\n",
    "        layer.trainable = False\n",
    "    # print trainable layers and it's index\n",
    "    for layer_number,layer in enumerate(model.layers):\n",
    "        if layer.trainable == True:\n",
    "            print(f'the layer in {model.name} at index {layer_number} is trainable {layer.trainable}')\n",
    "make_model_trainable(model_3.layers[2],10)\n",
    "# compile the model.\n",
    "model_3.compile(loss=CategoricalCrossentropy(),\n",
    "               optimizer=Adam(lr=0.0001),\n",
    "               metrics=['accuracy'])\n",
    "# fit the model 3\n",
    "initial_epochs = 5\n",
    "epochs = initial_epochs + 5\n",
    "history_3 = model_3.fit(train_ten_percent_data,epochs=epochs,\n",
    "                     steps_per_epoch=len(train_ten_percent_data),\n",
    "                     validation_data=test_ten_percent_data,\n",
    "                     validation_steps=int(0.25 *len(test_ten_percent_data)),\n",
    "                        initial_epoch = history_2.epoch[-1],\n",
    "                     callbacks=[tensorboard_callback_for_model_3,modelcheckpoint_callback_for_model_3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d63f6d2",
   "metadata": {},
   "source": [
    "## Model 4 - Same as Model 3 but with 100% of the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cffb0fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name_4 = 'model_4'\n",
    "# step 1 - create the base model\n",
    "base_model_4 = EfficientNetB0(include_top=False)\n",
    "# step 2 - Make the base model untrainable.\n",
    "base_model_4.trainable = False\n",
    "# step 3 - Create the input layer\n",
    "inputs_4 = Input(shape=INPUT_SHAPE,name='input_layer_1')\n",
    "# step 4 - create the data augmentation layer\n",
    "x = data_augmentation_layer(inputs_4)\n",
    "# step 5 - pass through the inputs to base model.\n",
    "x = base_model_4(x,training=False)\n",
    "# step 6 - pass throug the global max pool\n",
    "x = GlobalAveragePooling2D(name='global_average_pooling_layer')(x)\n",
    "# step 7 - define output layer\n",
    "outputs_4 = Dense(LABELS_SIZE,activation='softmax',name='output_layer')(x)\n",
    "# step 8 - create the model\n",
    "model_4 = tf.keras.Model(inputs_4,outputs_4)\n",
    "# step 9 - load weights from previous model check point file.\n",
    "model_4.load_weights(modelcheckpoint_path_2)\n",
    "\n",
    "# step 10 - create tensorboard call back\n",
    "log_dir = tensorboard_dir_name + '/' + experiment_name_4 + '/' + datetime.datetime.now().strftime('%Y%m%d-%H%M%S')\n",
    "tensorboard_callback_for_model_4 = TensorBoard(log_dir=log_dir)\n",
    "# step 11 - create model checkpoint callback\n",
    "modelcheckpoint_path_4 = f'{modelcheckpoint_dir_name}/{experiment_name_4}/checkpoint.ckpt' \n",
    "modelcheckpoint_callback_for_model_4 = ModelCheckpoint(filepath=modelcheckpoint_path_4, \n",
    "                                           save_weights_only=True,\n",
    "                                           save_best_only=False,\n",
    "                                           save_freq='epoch',\n",
    "                                           verbose=1)\n",
    "# step 10 - print the model summary\n",
    "model_4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a65e855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unfreeze top 10 layers in EfficientNetB0 for learning.\n",
    "def make_model_trainable(model,no_of_layers):\n",
    "    model.trainable = True\n",
    "    for layer in model.layers[:-10]:\n",
    "        layer.trainable = False\n",
    "    # print trainable layers and it's index\n",
    "    for layer_number,layer in enumerate(model.layers):\n",
    "        if layer.trainable == True:\n",
    "            print(f'the layer in {model.name} at index {layer_number} is trainable {layer.trainable}')\n",
    "make_model_trainable(model_4.layers[2],10)\n",
    "# compile the model.\n",
    "model_4.compile(loss=CategoricalCrossentropy(),\n",
    "               optimizer=Adam(lr=0.0001),\n",
    "               metrics=['accuracy'])\n",
    "# fit the model 3\n",
    "initial_epochs = 5\n",
    "epochs = initial_epochs + 5\n",
    "history_4 = model_4.fit(train_all_data,epochs=epochs,\n",
    "                     steps_per_epoch=len(train_all_data),\n",
    "                     validation_data=test_all_data,\n",
    "                     validation_steps=int(0.25 *len(test_all_data)),\n",
    "                        initial_epoch = history_2.epoch[-1],\n",
    "                     callbacks=[tensorboard_callback_for_model_4,modelcheckpoint_callback_for_model_4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "afd00a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_historys(original_history, new_history, initial_epochs=5):\n",
    "    \"\"\"\n",
    "    Compares two model history objects.\n",
    "    \"\"\"\n",
    "    # Get original history measurements\n",
    "    acc = original_history.history[\"accuracy\"]\n",
    "    loss = original_history.history[\"loss\"]\n",
    "\n",
    "    print(len(acc))\n",
    "\n",
    "    val_acc = original_history.history[\"val_accuracy\"]\n",
    "    val_loss = original_history.history[\"val_loss\"]\n",
    "\n",
    "    # Combine original history with new history\n",
    "    total_acc = acc + new_history.history[\"accuracy\"]\n",
    "    total_loss = loss + new_history.history[\"loss\"]\n",
    "\n",
    "    total_val_acc = val_acc + new_history.history[\"val_accuracy\"]\n",
    "    total_val_loss = val_loss + new_history.history[\"val_loss\"]\n",
    "\n",
    "    print(len(total_acc))\n",
    "    print(total_acc)\n",
    "\n",
    "    # Make plots\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(total_acc, label='Training Accuracy')\n",
    "    plt.plot(total_val_acc, label='Validation Accuracy')\n",
    "    plt.plot([initial_epochs-1, initial_epochs-1],\n",
    "              plt.ylim(), label='Start Fine Tuning') # reshift plot around epochs\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(total_loss, label='Training Loss')\n",
    "    plt.plot(total_val_loss, label='Validation Loss')\n",
    "    plt.plot([initial_epochs-1, initial_epochs-1],\n",
    "              plt.ylim(), label='Start Fine Tuning') # reshift plot around epochs\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ade0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://tensorboard.dev/experiment/VELvwU9kQKCLEYIMxH1WdQ/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
