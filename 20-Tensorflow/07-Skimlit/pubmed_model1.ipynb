{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55c153fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-01 09:34:05.393787: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pubmed-rct/PubMed_20k_RCT_numbers_replaced_with_at_sign/dev.txt', 'pubmed-rct/PubMed_20k_RCT_numbers_replaced_with_at_sign/test.txt', 'pubmed-rct/PubMed_20k_RCT_numbers_replaced_with_at_sign/train.txt']\n",
      "size of train lines is 210040\n",
      "size of validation lines is 35212\n",
      "size of test lines is 35135\n",
      "size of train data is 180040\n",
      "size of validation data is 30212\n",
      "size of test data is 30135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-01 09:34:15.695205: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-07-01 09:34:15.707851: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-07-01 09:34:15.708694: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-07-01 09:34:15.714093: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-07-01 09:34:15.714651: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-07-01 09:34:15.715341: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-07-01 09:34:18.246729: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-07-01 09:34:18.247363: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-07-01 09:34:18.247414: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1722] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2023-07-01 09:34:18.248077: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-07-01 09:34:18.248156: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3383 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/_label.py:116: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/_label.py:134: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/_label.py:134: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one hot encoding\n",
      "tf.Tensor([0. 0. 0. 1. 0.], shape=(5,), dtype=float64) tf.Tensor([1. 0. 0. 0. 0.], shape=(5,), dtype=float64) tf.Tensor([1. 0. 0. 0. 0.], shape=(5,), dtype=float64)\n",
      "label encoding...\n",
      "3 0 0\n",
      "classnames\n",
      "class name =  ['BACKGROUND' 'CONCLUSIONS' 'METHODS' 'OBJECTIVE' 'RESULTS'] length of classes = 5\n",
      "mean tokens in each sentences  26.338269273494777\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.preprocessing import OneHotEncoder,LabelEncoder\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# root data directory\n",
    "data_dir = 'pubmed-rct/PubMed_20k_RCT_numbers_replaced_with_at_sign/'\n",
    "# list of file names\n",
    "filenames = [data_dir + filename for filename in os.listdir(data_dir)]\n",
    "print(filenames)\n",
    "\n",
    "\n",
    "def get_lines(file_name):\n",
    "    \"\"\"\n",
    "    Reads the file_name and returns the content of the file as list\n",
    "    \"\"\"\n",
    "    with open(file_name) as f:\n",
    "        lines = f.readlines()\n",
    "    return lines\n",
    "\n",
    "validation_lines = get_lines(filenames[0])\n",
    "test_lines = get_lines(filenames[1])\n",
    "train_lines = get_lines(filenames[2])\n",
    "\n",
    "print(f\"size of train lines is {len(train_lines)}\")\n",
    "print(f\"size of validation lines is {len(validation_lines)}\")\n",
    "print(f\"size of test lines is {len(test_lines)}\")\n",
    "\n",
    "def create_data_set(lines):\n",
    "    data = []\n",
    "    items = []\n",
    "    index = 0\n",
    "    for line in lines:\n",
    "        \n",
    "        if line.startswith('\\n'):\n",
    "            for item in items:\n",
    "                item['total_lines'] = index -1\n",
    "                data.append(item)\n",
    "            index = 0\n",
    "            items = []\n",
    "        else:\n",
    "            tokens = line.split('\\t')\n",
    "        \n",
    "            if len(tokens) > 1:\n",
    "                item = {'line_number':index,'target': tokens[0],'text':tokens[1].strip() ,'total_lines' :index}\n",
    "                items.append(item)\n",
    "                index = index + 1\n",
    "    return data\n",
    "\n",
    "train_data = create_data_set(train_lines)\n",
    "validation_data = create_data_set(validation_lines)\n",
    "test_data = create_data_set(test_lines)\n",
    "\n",
    "print(f\"size of train data is {len(train_data)}\")\n",
    "print(f\"size of validation data is {len(validation_data)}\")\n",
    "print(f\"size of test data is {len(test_data)}\")\n",
    "\n",
    "train_df = pd.DataFrame(train_data)\n",
    "val_df   = pd.DataFrame(validation_data)\n",
    "test_df  = pd.DataFrame(test_data)\n",
    "\n",
    "train_sentences = train_df['text'].tolist()\n",
    "val_sentences   = val_df['text'].tolist()\n",
    "test_sentences  = test_df['text'].tolist()\n",
    "\n",
    "onehot = OneHotEncoder(sparse_output=False)\n",
    "train_label_one_hot = tf.constant(onehot.fit_transform(train_df['target'].to_numpy().reshape(-1,1)))\n",
    "val_label_one_hot    = tf.constant(onehot.transform(val_df['target'].to_numpy().reshape(-1,1)))\n",
    "test_label_one_hot   = tf.constant(onehot.transform(test_df['target'].to_numpy().reshape(-1,1)))\n",
    "print('one hot encoding')\n",
    "print(train_label_one_hot[0],val_label_one_hot[0],test_label_one_hot[0])\n",
    "\n",
    "# label encoder\n",
    "labelencode = LabelEncoder()\n",
    "train_label_encoded = labelencode.fit_transform(train_df['target'].to_numpy().reshape(-1,1))\n",
    "val_label_encoded = labelencode.transform(val_df['target'].to_numpy().reshape(-1,1))\n",
    "test_label_encoded = labelencode.transform(test_df['target'].to_numpy().reshape(-1,1))\n",
    "print('label encoding...')\n",
    "print(train_label_encoded[0],val_label_encoded[0],test_label_encoded[0])\n",
    "\n",
    "print('classnames')\n",
    "classnames = labelencode.classes_\n",
    "print('class name = ', classnames, 'length of classes =', len(classnames))\n",
    "\n",
    "# create text vectorizer\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "\n",
    "lengths = [len(sen.split()) for sen in train_sentences]\n",
    "print('mean tokens in each sentences ', np.mean(lengths))\n",
    "output_sequence_length = int(np.percentile(lengths,95))\n",
    "\n",
    "max_tokens = 68000\n",
    "text_vectorization = TextVectorization(max_tokens=max_tokens,\n",
    "                                      output_sequence_length=output_sequence_length)\n",
    "\n",
    "text_vectorization.adapt(train_sentences)\n",
    "rct_20k_text_vocab = text_vectorization.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2258f8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_sentences,train_label_one_hot))\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((val_sentences,val_label_one_hot))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_sentences,test_label_one_hot))\n",
    "\n",
    "# for fast performance \n",
    "train_dataset = train_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "val_dataset   = val_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "test_dataset  = test_dataset.batch(32).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aee2576d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(length, depth):\n",
    "    depth = depth/2\n",
    "    positions = np.arange(length)[:, np.newaxis]\n",
    "    depths = np.arange(depth)[np.newaxis, :] / depth\n",
    "\n",
    "    angle_rates = 1 / (10000 ** depths)\n",
    "    angle_rads = positions * angle_rates\n",
    "\n",
    "    pos_encoding = np.concatenate([np.sin(angle_rads), np.cos(angle_rads)],\n",
    "                                  axis=-1)\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca80f2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, vocab_size, d_model, length=2048):\n",
    "        super().__init__()\n",
    "        self.embedding = tf.keras.layers.Embedding(input_dim=vocab_size,\n",
    "                                                   output_dim=d_model,\n",
    "                                                   mask_zero=True)\n",
    "        self.positional_encoding = positional_encoding(length=length,\n",
    "                                                       depth=d_model)\n",
    "        \n",
    "        self.length = length\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = x + self.positional_encoding[tf.newaxis, :self.length, :]\n",
    "        return x\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'embedding':self.embedding,\n",
    "            'positional_encoding':self.positional_encoding,\n",
    "        }\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b540b0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64841, 55, 128)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_tokens = 68000\n",
    "lengths = [len(sen.split()) for sen in train_sentences]\n",
    "output_sequence_length = int(np.percentile(lengths,95))\n",
    "vocab_size = len(rct_20k_text_vocab)\n",
    "output_dim = 128\n",
    "output_units = 5\n",
    "sample_sentence = train_sentences[0]\n",
    "vocab_size,output_sequence_length,output_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "641f0162",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv1D, GlobalAveragePooling1D, Dense\n",
    "\n",
    "class PubMedTokenEmbeddingModel(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, max_tokens, output_seq_length, vocab_size, output_dim,output_units):\n",
    "        super().__init__()\n",
    "        self.text_vectorizer = TextVectorization(max_tokens=max_tokens, \n",
    "                                                 output_sequence_length=output_sequence_length,\n",
    "                                                name=\"vectorization_layer\")\n",
    "        self.text_vectorizer.adapt(train_sentences)\n",
    "        self.embedding = PositionalEmbedding(vocab_size=vocab_size,\n",
    "                                   d_model=output_dim,\n",
    "                                   length=output_sequence_length)\n",
    "#         self.embedding = Embedding(input_dim=len(rct_20k_text_vocab),\n",
    "#                      output_dim=128,\n",
    "#                      mask_zero=True,\n",
    "#                       name=\"token_embedding_layer\")\n",
    "        self.conv1D = Conv1D(filters=64,\n",
    "                             kernel_size=5,\n",
    "                             padding=\"same\",\n",
    "                             activation=\"relu\",\n",
    "                             name=\"conv1d_Layer\")\n",
    "        self.pooling = GlobalAveragePooling1D(name=\"global_average_pooling\")\n",
    "        self.outputs = Dense(output_units,activation=\"softmax\", name=\"fully_connected\")\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = self.text_vectorizer(inputs)\n",
    "        x = self.embedding(x)\n",
    "        x = self.conv1D(x)\n",
    "        x = self.pooling(x)\n",
    "        return self.outputs(x)\n",
    "    \n",
    "    def build_graph(self, raw_shape):\n",
    "        x = tf.keras.layers.Input(shape=raw_shape)\n",
    "        return Model(inputs=[x], outputs=self.call(x))\n",
    "    \n",
    "    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a209ef70",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_model = PubMedTokenEmbeddingModel(max_tokens=max_tokens,\n",
    "                                       output_seq_length=output_sequence_length,\n",
    "                                       vocab_size=vocab_size,\n",
    "                                       output_dim=output_dim,\n",
    "                                       output_units=output_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ede2660",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-01 09:35:11.068131: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:424] Loaded cuDNN version 8600\n",
      "2023-07-01 09:35:13.035635: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:637] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 5), dtype=float32, numpy=\n",
       "array([[0.22512673, 0.20956591, 0.21844925, 0.23885961, 0.10799854]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_model([sample_sentence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b9ad33ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 5), dtype=float32, numpy=\n",
       "array([[0.22512673, 0.20956591, 0.21844925, 0.23885961, 0.10799854]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_model(sample_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "357495c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_model.compile(loss=\"categorical_crossentropy\",\n",
    "               optimizer=tf.keras.optimizers.Adam(),\n",
    "               metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0f86b27a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-01 09:35:13.218311: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype double and shape [180040,5]\n",
      "\t [[{{node Placeholder/_1}}]]\n",
      "2023-07-01 09:35:14.476385: I tensorflow/compiler/xla/service/service.cc:169] XLA service 0x7f22d8009700 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-07-01 09:35:14.476474: I tensorflow/compiler/xla/service/service.cc:177]   StreamExecutor device (0): NVIDIA GeForce RTX 3060 Laptop GPU, Compute Capability 8.6\n",
      "2023-07-01 09:35:14.548024: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-07-01 09:35:15.067540: I ./tensorflow/compiler/jit/device_compiler.h:180] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "562/562 [==============================] - ETA: 0s - loss: 1.1245 - accuracy: 0.5292"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-01 09:36:15.868379: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype double and shape [30212,5]\n",
      "\t [[{{node Placeholder/_1}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "562/562 [==============================] - 64s 108ms/step - loss: 1.1245 - accuracy: 0.5292 - val_loss: 0.7472 - val_accuracy: 0.7078\n"
     ]
    }
   ],
   "source": [
    "token_model_history = token_model.fit(train_dataset,\n",
    "                                      steps_per_epoch=int(0.1*len(train_dataset)),\n",
    "                                      epochs=1,\n",
    "                                      validation_data=val_dataset,\n",
    "                                      validation_steps=int(0.1*len(val_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "502b0ab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "945/945 [==============================] - 10s 10ms/step - loss: 0.7500 - accuracy: 0.7056\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.7500398755073547, 0.7055805921554565]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_model.evaluate(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "63d20258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"pub_med_token_embedding_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " vectorization_layer (TextVe  multiple                 0         \n",
      " ctorization)                                                    \n",
      "                                                                 \n",
      " positional_embedding (Posit  multiple                 8299648   \n",
      " ionalEmbedding)                                                 \n",
      "                                                                 \n",
      " conv1d_Layer (Conv1D)       multiple                  41024     \n",
      "                                                                 \n",
      " global_average_pooling (Glo  multiple                 0         \n",
      " balAveragePooling1D)                                            \n",
      "                                                                 \n",
      " fully_connected (Dense)     multiple                  325       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 8,340,997\n",
      "Trainable params: 8,340,997\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "token_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8a2647ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "plot_model(token_model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e151cf4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_model.save_weights('resources/model/pubmed_model4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1d2ed602",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_model_new = PubMedTokenEmbeddingModel(max_tokens=max_tokens,\n",
    "                                       output_seq_length=output_sequence_length,\n",
    "                                       vocab_size=vocab_size,\n",
    "                                       output_dim=output_dim,\n",
    "                                       output_units=output_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d08568b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_model_new.compile(loss=\"categorical_crossentropy\",\n",
    "               optimizer=tf.keras.optimizers.Adam(),\n",
    "               metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "81ff52c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x7f25099fed30>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_model_new.load_weights('resources/model/pubmed_model4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4d07474a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "945/945 [==============================] - 11s 11ms/step - loss: 0.7500 - accuracy: 0.7056\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.7500398755073547, 0.7055805921554565]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_model_new.evaluate(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22021f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6e68b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
